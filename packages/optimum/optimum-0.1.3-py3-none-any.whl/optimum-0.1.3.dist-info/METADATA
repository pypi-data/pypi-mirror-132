Metadata-Version: 2.1
Name: optimum
Version: 0.1.3
Summary: Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.
Home-page: https://huggingface.co/hardware
Author: HuggingFace Inc. Special Ops Team
Author-email: hardware@huggingface.co
License: Apache
Keywords: transformers,quantization,pruning,training,ipu
Platform: UNKNOWN
Classifier: Development Status :: 3 - Alpha
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: coloredlogs
Requires-Dist: sympy
Requires-Dist: transformers (>=4.12.0)
Requires-Dist: torch (>=1.9)
Provides-Extra: graphcore
Requires-Dist: optimum-graphcore ; extra == 'graphcore'
Provides-Extra: intel
Requires-Dist: pycocotools ; extra == 'intel'
Requires-Dist: neural-compressor (>=1.7) ; extra == 'intel'
Requires-Dist: datasets (>=1.2.1) ; extra == 'intel'
Provides-Extra: onnxruntime
Requires-Dist: onnx ; extra == 'onnxruntime'
Requires-Dist: onnxruntime ; extra == 'onnxruntime'

[![ONNX Runtime](https://github.com/huggingface/optimum/actions/workflows/test-onnxruntime.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test-onnxruntime.yml)
[![neural_compressor](https://github.com/huggingface/optimum/actions/workflows/test-intel.yml/badge.svg)](https://github.com/huggingface/optimum/actions/workflows/test-intel.yml)

# Hugging Face - Optimum

ðŸ¤— Optimum is an extension of ðŸ¤— Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware.

The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.


## Integration with Hardware Partners  

ðŸ¤— Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models.

To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:
- [GraphCore IPUs](https://github.com/huggingface/optimum-graphcore) - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. [More information here](https://www.graphcore.ai/products/ipu)
- More to come soon! :star:

## Optimizing models towards inference

Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.


We currently support [ONNX runtime](https://github.com/microsoft/onnxruntime) along with [Intel Neural Compressor (INC)](https://github.com/intel/neural-compressor).

| Features                           | ONNX Runtime          | Intel Neural Compressor |
|:----------------------------------:|:---------------------:|:-----------------------:|
| Post-training Dynamic Quantization |  :heavy_check_mark:   |    :heavy_check_mark:   |  
| Post-training Static Quantization  |  Stay tuned! :star:   |    :heavy_check_mark:   |  
| Quantization Aware Training (QAT)  |        :x:            |    :heavy_check_mark:   |
| Pruning                            |        N/A            |    :heavy_check_mark:   |


## Install

ðŸ¤— Optimum can be installed using pip as follows:

`pip install optimum`

ðŸ¤— Optimum with [Intel Neural Compressor (INC)](https://github.com/intel/neural-compressor) or [ONNX runtime](https://github.com/microsoft/onnxruntime) dependencies can be installed respectively using pip as follows:

`pip install optimum[intel]`

`pip install optimum[onnxruntime]`

If you'd like to play with the examples or need the bleeding edge of the code and can't wait for a new release, you must install the library from source:

`pip install git+https://github.com/huggingface/optimum.git`


